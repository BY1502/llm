# # rag_pipeline/llm/json_answer.py

# from __future__ import annotations

# import json
# from dataclasses import dataclass
# from typing import Any, Dict, List

# from langchain_core.documents import Document
# from langchain_ollama import ChatOllama

# from rag_pipeline.config import ModelCfg  # ë„¤ê°€ ì“°ê³  ìˆëŠ” ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ë§ì¶°ì¤˜


# # ğŸ”¹ JSON + Auto-Schema ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
# AUTO_SCHEMA_SYSTEM_PROMPT = """
# ë‹¹ì‹ ì€ JSON êµ¬ì¡° ì„¤ê³„ì™€ ë°ì´í„° ë§¤í•‘ì„ ìˆ˜í–‰í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

# ê·œì¹™:
# 1. ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª… ë¬¸ì¥, ë§ˆí¬ë‹¤ìš´, ì£¼ì„ì„ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
# 2. ìµœìƒìœ„ í‚¤ëŠ” ë°˜ë“œì‹œ query, schema, answer, source_chunks ë„¤ ê°œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
# 3. schema.fields ì•ˆì˜ í•„ë“œ ëª©ë¡ì€ "ì‚¬ìš©ì ì§ˆë¬¸"ê³¼ "ì»¨í…ìŠ¤íŠ¸"ë¥¼ ë³´ê³  ë‹¹ì‹ ì´ ìŠ¤ìŠ¤ë¡œ ì„¤ê³„í•˜ì„¸ìš”.
# 4. answer ê°ì²´ëŠ” schema.fields ì •ì˜ì— ë§ê²Œë§Œ ê°’ì„ ì±„ìš°ì„¸ìš”.
# 5. source_chunksì—ëŠ” ì‹¤ì œë¡œ ì‚¬ìš©í•œ ê·¼ê±° ì²­í¬ë§Œ ìµœëŒ€ 5ê°œê¹Œì§€ ë„£ìœ¼ì„¸ìš”.
# 6. ìë£Œê°€ ì—†ëŠ” ê²½ìš°ì—ëŠ” "ìë£Œ ì—†ìŒ"ì´ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.

# ì¶œë ¥ JSON ìŠ¤í‚¤ë§ˆ:
# {
#   "query": string,                     
#   "schema": {
#     "description": string,             
#     "fields": [
#       {
#         "name": string,                
#         "type": "string | number | boolean | array | object",
#         "description": string
#       }
#     ]
#   },
#   "answer": object,                    
#   "source_chunks": [
#     {
#       "doc_id": string,
#       "chunk_id": string,
#       "snippet": string
#     }
#   ]
# }
# """


# def _build_context_block(docs: List[Document]) -> str:
#     """LLMì— ë„˜ê¸¸ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ ìƒì„± (ê¸°ì¡´ ë””ë²„ê·¸ ìŠ¤íƒ€ì¼ ìœ ì§€ ëŠë‚Œìœ¼ë¡œ)."""
#     lines: List[str] = []
#     for idx, d in enumerate(docs):
#         meta = d.metadata or {}
#         doc_id = (
#             meta.get("doc_id")
#             or meta.get("source")
#             or meta.get("doc")
#             or ""
#         )
#         chunk_id = meta.get("chunk_id") or meta.get("id") or f"chunk_{idx}"

#         header = f"[doc_id={doc_id} chunk_id={chunk_id}]"
#         content = d.page_content.strip().replace("\n", " ")
#         lines.append(f"{header} {content}")
#     return "\n".join(lines)


# def _parse_json_safely(raw: str) -> Dict[str, Any]:
#     """
#     Ollamaê°€ ì•ë’¤ì— ì•½ê°„ì˜ í…ìŠ¤íŠ¸ë¥¼ ë¶™ì´ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ì„œ
#     JSON ë¸”ë¡ë§Œ ì˜ë¼ë‚´ì„œ íŒŒì‹±í•˜ëŠ” ìœ í‹¸.
#     """
#     raw = raw.strip()

#     # ì´ë¯¸ ê¹¨ë—í•œ JSONì¼ ê°€ëŠ¥ì„± ìš°ì„  ì‹œë„
#     try:
#         return json.loads(raw)
#     except Exception:
#         pass

#     # ì²« ë²ˆì§¸ '{'ë¶€í„° ë§ˆì§€ë§‰ '}'ê¹Œì§€ ì˜ë¼ì„œ ì¬ì‹œë„
#     try:
#         start = raw.index("{")
#         end = raw.rindex("}") + 1
#         return json.loads(raw[start:end])
#     except Exception:
#         raise ValueError(f"LLM JSON íŒŒì‹± ì‹¤íŒ¨: {raw[:200]}...")


# def answer_with_json_autoschema(
#     query: str,
#     docs: List[Document],
#     model_cfg: ModelCfg,
#     llm_model: str | None = None,
# ) -> Dict[str, Any]:
#     """
#     JSON Mode + Auto-Schema ë°©ì‹ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” LLM í˜¸ì¶œ í•¨ìˆ˜.
#     - query: ì‚¬ìš©ì ì§ˆë¬¸
#     - docs: RAGë¡œ ì°¾ì€ top_k Document ë¦¬ìŠ¤íŠ¸
#     - model_cfg: ê¸°ì¡´ì— ì“°ëŠ” ModelCfg (ollama_model ì‚¬ìš©)
#     """
#     context_block = _build_context_block(docs)

#     user_prompt = f"""
# ì‚¬ìš©ì ì§ˆë¬¸: "{query}"

# ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸(ì²­í¬ë“¤):
# \"\"\"
# {context_block}
# \"\"\"

# ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•´ì„œ ë‹¤ìŒ ê·œì¹™ì„ ì§€í‚¤ë©´ì„œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.
# """

#     llm = ChatOllama(
#         model=model_cfg.ollama_model or model_cfg.ollama_model,
#         temperature=0.1,
#     )

#     # ğŸ”¹ Chat í˜•ì‹ ë©”ì‹œì§€ êµ¬ì„± (ë„¤ ìŠ¤íƒ€ì¼ì— ë§ê²Œ ë‹¨ìˆœí•˜ê²Œ)
#     messages = [
#         {"role": "system", "content": AUTO_SCHEMA_SYSTEM_PROMPT},
#         {"role": "user", "content": user_prompt},
#     ]

#     resp = llm.invoke(messages)
#     content = getattr(resp, "content", resp)

#     data = _parse_json_safely(content)
    
#     if "answer" not in data and "response" in data:
#         data["answer"] = data["response"]

#     # ìµœì†Œ ê²€ì¦
#     for key in ("query", "schema", "answer"):
#         if key not in data:
#             raise ValueError(f"LLM JSON ì‘ë‹µì— '{key}' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: {data}")

#     return data

# rag_pipeline/llm/json_answer.py

from __future__ import annotations

import json
from typing import Any, Dict, List

from langchain_core.documents import Document
from langchain_ollama import ChatOllama

from rag_pipeline.config import ModelCfg


# ğŸ”¹ RAG ë‹¨ìˆœ ë‹µë³€ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
AUTO_SCHEMA_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ê±´ì„¤ ì•ˆì „ ì‚¬ê³  ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” assistantì…ë‹ˆë‹¤.

ì—­í• :
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ í•¨ê»˜ ì£¼ì–´ì§€ëŠ” "ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸"ë¥¼ ë³´ê³ ,
  ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•©ë‹ˆë‹¤.
- ë°˜ë“œì‹œ ì»¨í…ìŠ¤íŠ¸ ì•ˆì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•´ì•¼ í•˜ë©°,
  íŒŒì¼ ë°–ì˜ ì§€ì‹ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.

ê·œì¹™:
1. ì‚¬ìš©ìê°€ íŠ¹ì • í•­ëª©(ì˜ˆ: êµ¬ì²´ì  ì‚¬ê³ ì›ì¸, ì¬ë°œë°©ì§€ ëŒ€ì±… ë“±)ì„ ë¬¼ì–´ë³´ë©´,
   ê·¸ í•­ëª©ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ë§Œ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
2. ì—¬ëŸ¬ ì‚¬ê³ ê°€ ì„ì—¬ ìˆë”ë¼ë„, ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ë„ê°€ ë†’ì€ í•œ ê±´ì˜ ì‚¬ê³ ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
3. ë‹µë³€ì€ ìì—°ì–´ ë¬¸ì¥ ë˜ëŠ” ì§§ì€ bullet í˜•íƒœë¡œ ì‘ì„±í•´ë„ ë©ë‹ˆë‹¤.
4. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ê³ ,
   "ì»¨í…ìŠ¤íŠ¸ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.
"""


def _build_context_block(docs: List[Document]) -> str:
    """
    LLMì— ë„˜ê¸¸ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ ìƒì„±.
    ê° Documentì˜ ë©”íƒ€ë°ì´í„°(doc_id, chunk_id)ì™€ ë‚´ìš©ì„ í•œ ì¤„ë¡œ ì •ë¦¬í•œë‹¤.
    """
    lines: List[str] = []
    for idx, d in enumerate(docs):
        meta = d.metadata or {}
        doc_id = (
            meta.get("doc_id")
            or meta.get("source")
            or meta.get("doc")
            or ""
        )
        chunk_id = meta.get("chunk_id") or meta.get("id") or f"chunk_{idx}"

        header = f"[doc_id={doc_id} chunk_id={chunk_id}]"
        content = d.page_content.strip().replace("\n", " ")
        lines.append(f"{header} {content}")
    return "\n".join(lines)


def _parse_json_safely(raw: str) -> Dict[str, Any]:
    """
    (í˜„ì¬ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ, í•„ìš” ì‹œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ JSON íŒŒì„œ)
    Ollamaê°€ ì•ë’¤ì— í…ìŠ¤íŠ¸ë¥¼ ë¶™ì´ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ì„œ
    JSON ë¸”ë¡ë§Œ ì˜ë¼ë‚´ì„œ íŒŒì‹±í•˜ëŠ” ìœ í‹¸.
    """
    raw = raw.strip()

    # ì´ë¯¸ ê¹¨ë—í•œ JSONì¼ ê°€ëŠ¥ì„± ìš°ì„  ì‹œë„
    try:
        return json.loads(raw)
    except Exception:
        pass

    # ì²« ë²ˆì§¸ '{'ë¶€í„° ë§ˆì§€ë§‰ '}'ê¹Œì§€ ì˜ë¼ì„œ ì¬ì‹œë„
    try:
        start = raw.index("{")
        end = raw.rindex("}") + 1
        return json.loads(raw[start:end])
    except Exception:
        raise ValueError(f"LLM JSON íŒŒì‹± ì‹¤íŒ¨: {raw[:200]}...")


def answer_with_json_autoschema(
    query: str,
    docs: List[Document],
    model_cfg: ModelCfg,
    llm_model: str | None = None,
) -> Dict[str, Any]:
    """
    ë‹¨ìˆœ RAG ë‹µë³€ í•¨ìˆ˜.

    Flow:
      1) ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ í•©ì³ì„œ
      2) ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ì§ˆë¬¸ + ì»¨í…ìŠ¤íŠ¸ë¥¼ LLMì— ì „ë‹¬
      3) LLMì´ ìƒì„±í•œ í•œêµ­ì–´ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ë°›ì•„ì„œ
      4) íŒŒì´í”„ë¼ì¸ í˜¸í™˜ì„ ìœ„í•´ JSON í˜•íƒœë¡œ ë˜í•‘í•´ì„œ ë°˜í™˜

    â€» ë” ì´ìƒ Auto-Schemaë¡œ í•„ë“œë¥¼ ì„¤ê³„í•˜ì§€ ì•Šê³ ,
       LLMì´ ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë³´ê³  ììœ ë¡­ê²Œ ë‹µë³€í•˜ë„ë¡ í•œë‹¤.
    """

    # 1) ì»¨í…ìŠ¤íŠ¸ ë¸”ë¡ ìƒì„±
    context_block = _build_context_block(docs)

    # 2) ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
    user_prompt = f"""
[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

[ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸]
\"\"\" 
{context_block}
\"\"\" 

ìœ„ ì»¨í…ìŠ¤íŠ¸ ë²”ìœ„ ì•ˆì—ì„œë§Œ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬,
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
ì§ˆë¬¸ì—ì„œ íŠ¹ì • í•­ëª©(ì˜ˆ: êµ¬ì²´ì  ì‚¬ê³ ì›ì¸, ì¬ë°œë°©ì§€ ëŒ€ì±… ë“±)ì„ ìš”êµ¬í•˜ë©´,
ê·¸ í•­ëª© ìœ„ì£¼ë¡œë§Œ ì •ë¦¬í•´ì„œ ë‹µë³€í•˜ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì •ë³´ê°€ ì—†ìœ¼ë©´, ì •ë³´ê°€ì—†ë‹¤ê³ ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

    llm = ChatOllama(
        model=llm_model or model_cfg.ollama_model,
        temperature=0.1,
    )

    messages = [
        {"role": "system", "content": AUTO_SCHEMA_SYSTEM_PROMPT.strip()},
        {"role": "user", "content": user_prompt.strip()},
    ]

    resp = llm.invoke(messages)
    content = getattr(resp, "content", resp)
    answer_text = str(content).strip()

    # 3) ì‚¬ìš©í•œ ê·¼ê±° ì²­í¬ (ìƒìœ„ ëª‡ ê°œë§Œ)
    source_chunks = []
    for i, d in enumerate(docs[:3]):
        md = d.metadata or {}
        source_chunks.append(
            {
                "doc_id": md.get("source", ""),
                "chunk_id": md.get("chunk_id") or md.get("id", f"chunk_{i}"),
                "snippet": d.page_content[:300],
            }
        )

    # 4) íŒŒì´í”„ë¼ì¸/í”„ë¡ íŠ¸ í˜¸í™˜ìš© JSON ë˜í•‘
    return {
        "query": query,
        "schema": {
            "description": "ìì—°ì–´ RAG ë‹µë³€",
            "fields": [
                {
                    "name": "answer",
                    "type": "string",
                    "description": "ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ í•œêµ­ì–´ ë‹µë³€",
                }
            ],
        },
        "answer": {
            "answer": answer_text,
        },
        "source_chunks": source_chunks,
    }
