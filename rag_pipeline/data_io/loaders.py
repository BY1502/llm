from __future__ import annotations

import csv
import logging
from pathlib import Path
from typing import List

import pandas as pd
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.language_models import BaseLanguageModel
from langchain_community.document_loaders import TextLoader, PyPDFLoader,PyMuPDFLoader

from rag_pipeline.config import ChunkCfg, PipelineCfg
from rag_pipeline.data_io.csv_schema import (
    FIELD_ALIASES,
    meta_get,
    make_csv_schema_report,
    print_csv_schema_report,
    save_csv_schema_report,
)
from rag_pipeline.data_io.readers import read_pdf, read_txt
from rag_pipeline.utils.text import extract_kv_metadata
from rag_pipeline.metadata.tag_extractor import extract_metadata_from_text

logger = logging.getLogger(__name__)

def _sanitize_metadata(meta: dict) -> dict:
    """
    Chromaì— ë„£ê¸° ì „ì— metadata ê°’ì„ primitive íƒ€ì…ìœ¼ë¡œ ì •ë¦¬.
    - str, int, float, bool, None: ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - list, tuple: ", "ë¡œ joiní•´ì„œ ë¬¸ìì—´ë¡œ ë³€í™˜
    - ê·¸ ì™¸ íƒ€ì…: str()ìœ¼ë¡œ ë³€í™˜
    """
    clean: dict = {}
    for k, v in meta.items():
        if isinstance(v, (str, int, float, bool)) or v is None:
            clean[k] = v
        elif isinstance(v, (list, tuple)):
            # tags ê°™ì€ ê²½ìš°: ['ê°€ì„¤ê³µì‚¬', 'ë™ë°”ë¦¬', ...] â†’ "ê°€ì„¤ê³µì‚¬, ë™ë°”ë¦¬, ..."
            clean[k] = ", ".join(str(x) for x in v)
        else:
            clean[k] = str(v)
    return clean



def _try_read_csv(path):
    # 1) íŒŒì¼ ì•ë¶€ë¶„ ìƒ˜í”Œ
    raw = Path(path).read_bytes()
    head = raw[:4096]

    # 2) í›„ë³´ ì¸ì½”ë”©/êµ¬ë¶„ì
    encodings = ["cp949", "utf-8", "utf-8-sig", "euc-kr", "ISO-8859-1"]
    seps = [",", ";", "\t"]     # ì½¤ë§ˆ/ì„¸ë¯¸ì½œë¡ /íƒ­

    # 3) pandas ìœ ì—° ì˜µì…˜
    common_kwargs = dict(
        engine="python",           # ë” ê´€ëŒ€í•¨
        on_bad_lines="skip",       # ê¹¨ì§„ ë¼ì¸ ê±´ë„ˆëœ€ (pandas>=1.5)
        dtype=str,                 # ëª¨ë“  ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ
        quoting=csv.QUOTE_MINIMAL, # ë”°ì˜´í‘œ ì²˜ë¦¬
    )

    # 4) sep ìë™ ì¶”ì • 1ì°¨ (csv.Sniffer)
    sniff_sep = None
    try:
        sample = head.decode("utf-8", errors="ignore")
        sniff = csv.Sniffer().sniff(sample, delimiters=";,\t")
        sniff_sep = sniff.delimiter
    except Exception:
        pass

    # 5) ì‹œë„ ìˆœì„œ: (ì¶”ì • sep + ê° ì¸ì½”ë”©) â†’ (seps ì „ìˆ˜ + ê° ì¸ì½”ë”©) â†’ (sep=None)
    # 5-1) ì¶”ì • sepê°€ ìˆìœ¼ë©´ ë¨¼ì € ì‹œë„
    if sniff_sep:
        for enc in encodings:
            try:
                logger.debug("CSV sniff attempt: path=%s sep=%r enc=%s", path, sniff_sep, enc)
                return pd.read_csv(path, encoding=enc, sep=sniff_sep, **common_kwargs)
            except Exception as e:
                logger.debug(
                    "CSV sniff failed: path=%s sep=%r enc=%s error=%s",
                    path,
                    sniff_sep,
                    enc,
                    e,
                )

    # 5-2) ëŒ€í‘œ êµ¬ë¶„ìë“¤ ì „ìˆ˜ ì‹œë„
    for sep in seps:
        for enc in encodings:
            try:
                logger.debug("CSV attempt: path=%s sep=%r enc=%s", path, sep, enc)
                return pd.read_csv(path, encoding=enc, sep=sep, **common_kwargs)
            except Exception as e:
                logger.debug(
                    "CSV failed: path=%s sep=%r enc=%s error=%s",
                    path,
                    sep,
                    enc,
                    e,
                )
                # pass

    # 5-3) ë§ˆì§€ë§‰ ì‹œë„: sep ìë™(None) + ì¸ì½”ë”© ì „ìˆ˜
    for enc in encodings:
        try:
            logger.debug("CSV fallback attempt: path=%s sep=auto enc=%s", path, enc)
            return pd.read_csv(path, encoding=enc, sep=None, **common_kwargs)
        except Exception as e:
            logger.debug(
                "CSV fallback failed: path=%s sep=auto enc=%s error=%s",
                path,
                enc,
                e,
            )
            pass

    logger.error("CSV parsing failed for %s", path)
    return None

# def csv_rows_to_documents(path: Path, pipeline_cfg: PipelineCfg) -> List[Document]:
#     df = _try_read_csv(path)
#     if df is None or df.empty:
#         raise RuntimeError(f"Failed to read CSV (encoding/sep/lines): {path}")

#     df.columns = [str(c).strip() for c in df.columns]

#     report = make_csv_schema_report(df, path)
#     print_csv_schema_report(report)
#     save_csv_schema_report(
#         report,
#         str(pipeline_cfg.schema_report_dir) if pipeline_cfg.schema_report_dir else None,
#     )

#     # FIELD_ALIASESì˜ logical key ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ, ì‹¤ì œ ì»¬ëŸ¼ëª…ì€ meta_getì´ ì•Œì•„ì„œ ë§¤í•‘í•œë‹¤.
#     logical_keys = list(FIELD_ALIASES.keys())

#     docs: List[Document] = []
#     for i, (_, row) in enumerate(df.iterrows()):
#         # 1) ì›ë³¸ rowë¥¼ ê·¸ëŒ€ë¡œ ë©”íƒ€ë°ì´í„°ì— ë„£ëŠ”ë‹¤ (íŒŒì¼ì— ìˆëŠ” ì»¬ëŸ¼ëª… ê·¸ëŒ€ë¡œ).
#         md = {str(k).strip(): ("" if pd.isna(v) else str(v)) for k, v in row.items()}

#         # 2) ë…¼ë¦¬ í‚¤ ê¸°ì¤€ìœ¼ë¡œ ê°’ ì¶”ì¶œ (alias í¬í•¨).
#         body_lines: list[str] = []
#         normalized_meta: dict = {}
#         for key in logical_keys:
#             val = meta_get(md, key)
#             if val and val != "ì •ë³´ ì—†ìŒ":
#                 body_lines.append(f"{key}: {val}")
#                 # ë©”íƒ€ë°ì´í„°ì—ë„ í‘œì¤€í™”ëœ í‚¤ë¡œ í•œ ë²ˆ ë” ì €ì¥í•´ ë‘”ë‹¤.
#                 normalized_meta[key] = val

#         body = "\n".join(body_lines).strip()

#         # 3) ìµœì¢… ë©”íƒ€ë°ì´í„°: source + ì›ë³¸ ì»¬ëŸ¼ + í‘œì¤€í™” í‚¤ + ê³ ì • id
#         meta = {
#             "source": str(path),
#             **md,
#             **normalized_meta,
#         }
#         meta["id"] = f"{meta['source']}#p0#c{i}"

#         docs.append(
#             Document(
#                 page_content=body or str(md),
#                 metadata=meta,
#             )
#         )

#     return docs

# ëª¨ë“  ì¹¼ëŸ¼ì„ ë©”íƒ€ë°ì´í„°ë¡œ ì¶”ì¶œ
def csv_rows_to_documents(path: Path, pipeline_cfg: PipelineCfg) -> List[Document]:
    df = _try_read_csv(path)
    if df is None or df.empty:
        raise RuntimeError(f"Failed to read CSV (encoding/sep/lines): {path}")

    # í—¤ë” ê³µë°± ì •ë¦¬
    df.columns = [str(c).strip() for c in df.columns]

    # (ì„ íƒ) ìŠ¤í‚¤ë§ˆ ë¦¬í¬íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
    report = make_csv_schema_report(df, path)
    print_csv_schema_report(report)
    save_csv_schema_report(
        report,
        str(pipeline_cfg.schema_report_dir) if pipeline_cfg.schema_report_dir else None,
    )

    docs: List[Document] = []

    for i, (_, row) in enumerate(df.iterrows()):
        # 1) ì´ rowì˜ ë©”íƒ€ë°ì´í„°: CSV ì»¬ëŸ¼ëª…ì„ ê·¸ëŒ€ë¡œ í‚¤ë¡œ ì‚¬ìš©
        md: dict[str, str] = {}
        for k, v in row.items():
            col_name = str(k).strip()
            # NaN/ë¹ˆ ë¬¸ìì—´ì€ ì œì™¸
            if pd.isna(v):
                continue
            val = str(v).strip()
            if not val:
                continue
            md[col_name] = val

        # 2) page_content = "ì»¬ëŸ¼ëª…: ê°’" ì¤„ë¡œ ì­‰ ì´ì–´ë¶™ì´ê¸°
        body_lines: list[str] = [f"{key}: {val}" for key, val in md.items()]
        body = "\n".join(body_lines).strip()

        # 3) ìµœì¢… ë©”íƒ€ë°ì´í„°: source + ì›ë³¸ ì»¬ëŸ¼ ê·¸ëŒ€ë¡œ + id
        meta = {
            "source": str(path),
            **md,
        }
        meta["id"] = f"{meta['source']}#p0#c{i}"

        docs.append(
            Document(
                page_content=body or str(md),
                metadata=meta,
            )
        )

    return docs


def load_documents_from_path(path: Path, chunk_cfg: ChunkCfg, pipeline_cfg: PipelineCfg,
                             llm_for_tags: BaseLanguageModel | None = None) -> List[Document]:
    if path.is_dir():
        docs: List[Document] = []
        for p in path.rglob("*"):
            if p.is_file() and p.suffix.lower() in {".csv", ".txt", ".pdf"}:
                logger.info("Loading file for chunking: %s", p)
                docs.extend(load_documents_from_path(p, chunk_cfg, pipeline_cfg, llm_for_tags=llm_for_tags))
        return docs
    if path.suffix.lower() == ".csv":
        return csv_rows_to_documents(path, pipeline_cfg)
    elif path.suffix.lower() in {".txt", ".pdf"}:
        return generic_file_to_documents(path, chunk_cfg,llm_for_tags=llm_for_tags)
    return []


def generic_file_to_documents(
    path: Path,
    chunk_cfg: ChunkCfg,
    llm_for_tags: BaseLanguageModel | None = None,
) -> List[Document]:
    print(f"[CHUNK] generic_file_to_documents start: {path}")
    """
    TXT/PDF ë“± ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì²­í‚¹í•´ì„œ LangChain Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜.
    + (ì˜µì…˜) llm_for_tags ê°€ ì£¼ì–´ì§€ë©´ ë¬¸ì„œ ë ˆë²¨ íƒœê·¸/ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•´ì„œ ê° ì²­í¬ metadataì— í¬í•¨.
    """

    # 1) ì›ë³¸ í…ìŠ¤íŠ¸ ì½ê¸° (í™•ì¥ì ê¸°ì¤€)
    ext = path.suffix.lower()
    if ext == ".txt":
        raw = read_txt(path)
    elif ext == ".pdf":
        loader = PyMuPDFLoader(str(path))
        pages = loader.load()
        raw = "\n".join(p.page_content for p in pages)
        print(f"[PDF] PyMuPDFLoader loaded {len(pages)} pages from {path.name}")
    else:
        # raw = ""
        return [] # ì¶”ê°€

    if not raw or not raw.strip():
        return []

    # 2) ê¸°ë³¸ ë©”íƒ€ë°ì´í„° (ê³µí†µ)
    base_meta: dict = {
        "source": str(path),
    }

    # 3) LLMìœ¼ë¡œ íƒœê·¸/ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    if llm_for_tags is not None:
        try:
            tag_meta = extract_metadata_from_text(
                text=raw[:4000],
                filename=path.name,
                llm=llm_for_tags,
            )
        except Exception as e:
            print(f"[TAG] ë©”íƒ€ë°ì´í„°/íƒœê·¸ ì¶”ì¶œ ì‹¤íŒ¨: file={path} | err={e}")
            tag_meta = {}
        else:
            base_meta.update({
            "document_type": tag_meta.get("document_type"),
            "project_name": tag_meta.get("project_name"),
            "location": tag_meta.get("location"),
            "company": tag_meta.get("company"),
            "facility_type": tag_meta.get("facility_type"),
            "tags": tag_meta.get("tags", []),
            "source_filename": tag_meta.get("source_filename", path.name),
        })

    # 4) í‚¤:ê°’ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    kv_meta = extract_kv_metadata(raw)
    base_meta.update({f"meta:{k}": v for k, v in kv_meta.items()})

    # 5) ì²­í‚¹
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_cfg.size,
        chunk_overlap=chunk_cfg.overlap,
        separators=["\n\n", "\n", ". ", ".", " "],
    )
    chunks = splitter.split_text(raw)

    print(
        f"[CHUNK] file={path.name} ext={path.suffix.lower()} "
        f"chunks={len(chunks)} size={chunk_cfg.size} overlap={chunk_cfg.overlap}"
    )

        # 6) Document ë¦¬ìŠ¤íŠ¸ ìƒì„±
    docs: List[Document] = []
    for i, ch in enumerate(chunks):
        raw_meta = {
            **base_meta,
            "chunk_id": i,
            "id": f"{base_meta['source']}#p0#c{i}",
        }

        # ğŸ”¥ Chromaê°€ ë¨¹ì„ ìˆ˜ ìˆê²Œ ë©”íƒ€ë°ì´í„° ì •ë¦¬
        safe_meta = _sanitize_metadata(raw_meta)

        docs.append(
            Document(
                page_content=ch,
                metadata=safe_meta,
            )
        )

    return docs



