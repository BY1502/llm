from rag_pipeline.config import ModelCfg
from rag_pipeline.retrieval.hybrid import hybrid_retrieve
from rag_pipeline.retrieval.rerankers import CrossEncoderReranker
from rag_pipeline.indexing.sparse import build_sparse_retriever
from rag_pipeline.llm.json_answer import answer_with_json_autoschema
from rag_pipeline.llm.extract_filters import extract_filters
from rag_pipeline.core.global_index import get_global_index
from rag_pipeline.core.workspace import WORKSPACES
from rag_pipeline.core.processing import VS_CACHE, BM25_CACHE, DOCS_CACHE
from fastapi import HTTPException
from langchain_core.documents import Document
import logging
from typing import Any
logger = logging.getLogger(__name__)


# ---------------------------
#  Helper: tags parser
# ---------------------------
def _ensure_tags_list(raw):
    """
    metadata['tags'] ê°’ì´
    - string: "ê°€ì„¤ê³µì‚¬, ë¹„ê³„, ì¶”ë½ ìœ„í—˜"
    - list: ["ê°€ì„¤ê³µì‚¬","ë¹„ê³„"]
    ì–´ëŠ í˜•íƒœë“  list[str] ë¡œ ë³€í™˜.
    """
    if isinstance(raw, str):
        return [t.strip() for t in raw.split(",") if t.strip()]
    if isinstance(raw, list):
        return raw
    return []

CASE_QUERY_STOPWORDS = [
    "ì‚¬ê³ ", "ì‚¬ë¡€", "ì‚¬ê³ ì‚¬ë¡€", "ì‚¬ê³  ì‚¬ë¡€",
    "ì•Œë ¤ì¤˜", "ì•Œë ¤ ì¤˜", "ì•Œë ¤ì¤˜ìš”",
    "ì— ëŒ€í•œ", "ì—ëŒ€í•œ", "ì— ê´€í•´", "ì—ê´€í•´",
    "ì„", "ë¥¼", "ì´", "ê°€", "ì€", "ëŠ”"
]

def extract_query_keywords(query: str | None) -> list[str]:
    """
    ì§ˆë¬¸ì—ì„œ 'ìˆœì˜ì¢…í•©ê±´ì„¤', 'ìŠ¤íƒ€í•„ë“œì•ˆì„±', 'ì§€ì‹ì‚°ì—…ì„¼íƒ€' ê°™ì€
    ê³ ìœ ëª…ì‚¬/í•µì‹¬ í‚¤ì›Œë“œë§Œ ëŒ€ëµ ë½‘ëŠ”ë‹¤.
    'ìš°ì„ ìˆœìœ„ íŒíŠ¸' ë¡œ ì‚¬ìš©
    """
    if not query:
        return []
    q = query.strip()
    if not q:
        return []
    
    parts = q.split()
    keywords: list[str] = []
    for part in parts:
        part = part.strip()
        if not part:
            continue
        
        if any(sw in part for sw in CASE_QUERY_STOPWORDS):
            continue
        if len(part) < 2:
            continue
        
        keywords.append(part)
        
    return keywords

def prioritize_docs_by_keywords(docs: list[Document], keywords: list[str]) -> list[Document]:
    """
    ë¬¸ì„œë¥¼ ë²„ë¦¬ì§€ ì•Šê³ , í‚¤ì›Œë“œê°€ ë§ì´ í¬í•¨ëœ ë¬¸ì„œì¼ìˆ˜ë¡ ë¦¬ìŠ¤íŠ¸ì˜ ì•ìª½ìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.
    (ë‹¨ìˆœ ìœ /ë¬´ê°€ ì•„ë‹ˆë¼, ë§¤ì¹­ëœ ê°œìˆ˜(Count)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬)
    """
    if not keywords or not docs:
        return docs
    
    # ë¬¸ì„œë³„ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
    scored_docs = []
    for d in docs:
        md = d.metadata or {}
        # ê²€ìƒ‰ ëŒ€ìƒ í…ìŠ¤íŠ¸ ìƒì„± (ë©”íƒ€ë°ì´í„° + ë³¸ë¬¸)
        # None ê°’ í•„í„°ë§ ë° ë¬¸ìì—´ ë³€í™˜
        meta_values = [str(v) for v in md.values() if v is not None]
        text_pieces = meta_values + [
            str(md.get("source", "") or ""),
            str(d.page_content or ""),
        ]
        big_text = " ".join(text_pieces)
        
        # ğŸ”¥ [í•µì‹¬ ìˆ˜ì •] í‚¤ì›Œë“œê°€ 'ëª‡ ê°œ'ë‚˜ í¬í•¨ë˜ì—ˆëŠ”ì§€ ì¹´ìš´íŠ¸ (ì ìˆ˜í™”)
        match_count = sum(1 for kw in keywords if kw in big_text)
        
        # (ë§¤ì¹­ ê°œìˆ˜, ì›ë˜ ìˆœì„œ ë³´ì¡´ì„ ìœ„í•œ ë¬¸ì„œ ê°ì²´)
        scored_docs.append((match_count, d))
        
    # ë§¤ì¹­ ê°œìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ë§ì€ ê²Œ ìœ„ë¡œ)
    # íŒŒì´ì¬ì˜ sortëŠ” stableí•˜ë¯€ë¡œ, ì ìˆ˜ê°€ ê°™ìœ¼ë©´ ì›ë˜(Vector/BM25) ìˆœìœ„ê°€ ìœ ì§€ë¨
    scored_docs.sort(key=lambda x: x[0], reverse=True)
    
    # ì •ë ¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    return [d for count, d in scored_docs]
def run_pipeline(req):

    model_cfg = ModelCfg()

    # ğŸ”¹ LLM ëª¨ë¸ ì„ íƒ
    llm_choise = getattr(req, "llm_model", None) or "gemma3:27b"
    print(f"[LLM MODEL] : {llm_choise}")

    # ì§ˆì˜ ì •ë³´ ë¡œê·¸
    print(f"[QUERY] query={req.query} | workspace={req.workspace_id} | tags={req.tags}")

    reranker = CrossEncoderReranker(model_cfg.rerank_model) if req.use_rerank else None

    vs = bm25 = docs = None
    ws_id = getattr(req, "workspace_id", None)

    # -----------------------------------------------------
    # 1) workspace ìºì‹œ ê¸°ë°˜ ë¡œë“œ
    # -----------------------------------------------------
    if ws_id:
        ws = WORKSPACES.get(ws_id)
        if not ws:
            raise HTTPException(400, "ìœ íš¨í•˜ì§€ ì•Šì€ workspace_id ì…ë‹ˆë‹¤.")

        vs = VS_CACHE.get(ws_id)
        bm25 = BM25_CACHE.get(ws_id)
        docs = DOCS_CACHE.get(ws_id)

        print(
            f"[DEBUG] ws_id={ws_id}, "
            f"vs_in_cache={ws_id in VS_CACHE}, "
            f"bm25_in_cache={ws_id in BM25_CACHE}, "
            f"docs_in_cache={ws_id in DOCS_CACHE}, "
            f"docs_len={len(docs) if docs else 0}"
        )

    # -----------------------------------------------------
    # 2) ì „ì—­ ì¸ë±ìŠ¤ fallback
    # -----------------------------------------------------
    if vs is None and bm25 is None and docs is None:
        try:
            g_vs, g_bm25, g_docs = get_global_index()
            vs = g_vs or vs
            bm25 = g_bm25 or bm25
            docs = g_docs or docs
            print(
                f"[DEBUG] global_index: vs={bool(vs)}, "
                f"bm25={bool(bm25)}, docs_len={len(docs) if docs else 0}"
            )
        except Exception:
            print("[DEBUG] get_global_index() ì‹¤íŒ¨, ì „ì—­ ì¸ë±ìŠ¤ ì‚¬ìš© ì•ˆ í•¨")

    # -----------------------------------------------------
    # 3) BM25 lazy build
    # -----------------------------------------------------
    if bm25 is None and docs:
        bm25 = build_sparse_retriever(docs)
        if ws_id:
            BM25_CACHE[ws_id] = bm25
        print(f"[DEBUG] lazy build bm25, docs={len(docs)}")
        
    # -----------------------------------------------------
    # 3.5) Pre-filtering ì¡°ê±´ ì¶”ì¶œ
    # -----------------------------------------------------
    
    search_filter = None
    # ì§ˆë¬¸ì´ ë„ˆë¬´ ì§§ì§€ ì•Šì„ ë•Œë§Œ í•„í„° ì¶”ì¶œ ì‹œë„ (ë¹„ìš©/ì†ë„ ê³ ë ¤)
    if req.query and len(req.query) > 5:
        try:
            search_filter = extract_filters(req.query, model_cfg)
        except Exception as e:
            print(f"[FILTER] í•„í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # -----------------------------------------------------
    # 4) retrieval (dense / sparse / hybrid)
    # -----------------------------------------------------
    used_vs = False
    used_bm25 = False

    mode = (req.retrieval or "hybrid").lower()
    
    CANDIDATE_K = max(req.final_k * 3, 10)

    if mode == "dense":
        # Dense ëª¨ë“œì—ë„ í•„í„° ì ìš©
        dense_kwargs = {"k": req.k}
        if search_filter:
            dense_kwargs["filter"] = search_filter
        matched = vs.as_retriever(search_kwargs={"k": req.k}).invoke(req.query) if vs else []
        used_vs = True

    elif mode == "sparse":
        if bm25:
            bm25.k = req.k
            matched = bm25.invoke(req.query)
            used_bm25 = True
        else:
            matched = []

    else:  # hybrid
        if vs and bm25:
            matched = hybrid_retrieve(
                req.query,
                vs,
                bm25,
                k_dense=req.k,
                k_sparse=req.k,
                # k_final=req.final_k,
                k_final=CANDIDATE_K,
                reranker=reranker,
                filter=search_filter, # ì¶”ì¶œí•œ í•„í„° ì „ë‹¬
            )
            used_vs = True
            used_bm25 = True
        elif vs:
            matched = vs.as_retriever(search_kwargs={"k": req.k}).invoke(req.query)
            used_vs = True
        elif bm25:
            bm25.k = req.k
            matched = bm25.invoke(req.query)
            used_bm25 = True
        else:
            matched = []

    print(f"[DEBUG] mode={mode}, vs={used_vs}, bm25={used_bm25}, matched_len={len(matched)}")
    print(
        f"[DEBUG] query={req.query!r}, mode={mode}, "
        f"k={req.k}, final_k={req.final_k}, use_rerank={req.use_rerank}, "
    )

    # -----------------------------------------------------
    # 5) ë§¤ì¹­ëœ ë¬¸ì„œë“¤ì˜ íƒœê·¸ ë¶„í¬ ì¶œë ¥
    # -----------------------------------------------------
    all_tags = []
    for d in matched:
        all_tags.extend(_ensure_tags_list(d.metadata.get("tags")))

    # ğŸ”¥ [ì£¼ì„ ì²˜ë¦¬] ë§¤ì¹­ëœ ë°ì´í„° ì¶œë ¥ ( ë„ˆë¬´ ê¸¸ì–´ì„œ ì¶œë ¥í•˜ì§€ ì•ŠìŒ )
    # unique_tags = sorted(set(all_tags))
    # print(f"[MATCH] matched_docs={len(matched)} | unique_tags={unique_tags}")

    # if matched:
    #     print("[DEBUG] first matched metadata:", matched[0].metadata)

    # -----------------------------------------------------
    # 6) ìš”ì²­ì—ì„œ tags í•„í„°ë§
    # -----------------------------------------------------
    req_tags = getattr(req, "tags", None)
    if req_tags:
        required_tags = set(req_tags)
        before = len(matched)

        matched = [
            d for d in matched
            if required_tags.intersection(_ensure_tags_list(d.metadata.get("tags")))
        ]

        print(f"[TAG-FILTER] required={list(required_tags)} | before={before} -> after={len(matched)}")

    # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)
    if not matched:
        return {
            "summary": "ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            "sources": [],
        }

    # ----------------------------------------------------
    # 6.5) í‚¤ì›Œë“œ ê¸°ë°˜ 'ìˆœì„œë§Œ' ì¡°ì • (ë¬¸ì„œ ë²„ë¦¬ì§€ ì•ŠìŒ)
    # ----------------------------------------------------
    keywords = extract_query_keywords(getattr(req, "query", None))
    ordered = prioritize_docs_by_keywords(matched, keywords)

    print(
        # f"[LLM MODEL] : {llm_choise}"
        f"[ORDER] keywords={keywords} | before={len(matched)} | "
        f"first_changed={matched[0] is not ordered[0] if matched and ordered else False}"
    )

    # ğŸ”¹ LLMì— ë„˜ê¸¸ ìµœì¢… ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ì •ë ¬ ì ìš©, ì‹¤íŒ¨ ì‹œ fallback)
    # final_docs = ordered or matched
    final_docs = ordered[:req.final_k] if ordered else matched[:req.final_k]

    print(
        f"[ORDER] keywords={keywords} | candidates={len(ordered)} -> final_k={len(final_docs)} | "
        f"first_changed={matched[0] is not ordered[0] if matched and ordered else False}"
    )

    # -----------------------------------------------------
    # 7) LLM JSON Auto-Schema ì‘ë‹µ ìƒì„±
    # -----------------------------------------------------

    # âœ… JSON Mode + Auto-Schema í˜¸ì¶œ
    json_answer = answer_with_json_autoschema(
        query=req.query,
        docs=final_docs,
        model_cfg=model_cfg,
        llm_model=llm_choise,   # ìš”ì²­ì—ì„œ ë°›ì€ LLM ì„ íƒ ë°˜ì˜
    )

    # -----------------------------------------------------
    # 8) ë°˜í™˜ (ê¸°ì¡´ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ìœ ì§€ + JSON Auto-Schema ì¶”ê°€)
    # -----------------------------------------------------

    # summary í•„ë“œëŠ” response ëª¨ë¸ì´ ìš”êµ¬í•˜ë‹ˆê¹Œ, ê°„ë‹¨íˆ schema.descriptionì„ ì‚¬ìš©
    summary_text = json_answer.get("schema", {}).get("description", "")

    return {
        "summary": summary_text,          # ğŸ”¹ FastAPI ì‘ë‹µ ìŠ¤í‚¤ë§ˆìš© (string)
        "mode": mode,
        "llm_model": llm_choise,
        "json_data": json_answer,              # ğŸ”¹ ìƒˆ JSON Auto-Schema ì „ì²´
        "sources": [
            {
                "source": d.metadata.get("source", ""),
                "doc_id": d.metadata.get("doc_id") or d.metadata.get("source") or "",
                "chunk_id": d.metadata.get("chunk_id") or d.metadata.get("id") or "",
            }
            for d in final_docs
        ],
    }

