# from __future__ import annotations
# import time
# import logging
# from typing import List

# import chromadb
# from langchain_huggingface import HuggingFaceEmbeddings
# from langchain_chroma import Chroma
# from langchain_core.documents import Document

# from ..config import ModelCfg, IndexCfg
# from ..utils.ids import make_chunk_id

# logger = logging.getLogger(__name__)

# def build_vectorstore(docs: List[Document], model_cfg: ModelCfg, idx_cfg: IndexCfg) -> Chroma:
#     """
#     ê¸°ì¡´ add_documents -> Chroma ë„¤ì´í‹°ë¸Œ upsert ë¡œ ë³€ê²½.
#     - ê°™ì€ idë©´ ê°±ì‹ , ì—†ìœ¼ë©´ ì‚½ì… (ì¤‘ë³µ ë°©ì§€)
#     - upsert ì´í›„, ê²€ìƒ‰ì„ ìœ„í•´ LangChain Chroma ë˜í¼ë¥¼ ë°˜í™˜
#     """
#     # 1) Chroma client & collection
#     client = chromadb.PersistentClient(path=str(idx_cfg.chroma_dir))
#     coll = client.get_or_create_collection(name=idx_cfg.collection)  # <-- upsert ì§€ì›

#     # 2) Embedding ì¤€ë¹„ (ì§ì ‘ ê³„ì‚°í•´ì„œ upsertì— embeddingsë¡œ ì „ë‹¬)
#     embeddings = HuggingFaceEmbeddings(
#         model_name=model_cfg.embed_model,
#         encode_kwargs={"normalize_embeddings": True}
#     )

#     if docs:
#         BATCH = 128
#         total = len(docs)
#         done = 0
#         t0 = time.perf_counter()

#         logger.info(
#             "Vectorstore upsert start: docs=%d batch=%d collection=%s dir=%s",
#             total, BATCH, idx_cfg.collection, idx_cfg.chroma_dir
#         )

#         # Document -> (id, text, metadata) ë³€í™˜
#         def _to_tuple(d: Document):
#             meta = dict(getattr(d, "metadata", {}) or {})
#             source = meta.get("source") or meta.get("doc_id") or "unknown"
#             page = meta.get("page")
#             text = d.page_content or ""
#             _id = make_chunk_id(str(source), page, text)
#             # ë””ë²„ê¹… í¸ì˜ë¥¼ ìœ„í•´ ë©”íƒ€ì—ë„ ì‹¬ì–´ë‘ê¸°
#             meta.setdefault("chunk_id", _id)
#             return _id, text, meta

#         seen_ids = set()
#         tuples = []
#         for d in docs:
#             _id, text, meta = _to_tuple(d)
#             if _id in seen_ids:
#                 # í•„ìš”í•˜ë©´ logger ë¡œ ì°ì–´ë„ ë¨
#                 # logger.warning("Duplicate chunk id detected, skip: %s (source=%s)", _id, meta.get("source"))
#                 continue
#             seen_ids.add(_id)
#             tuples.append((_id, text, meta))

#         for s in range(0, total, BATCH):
#             batch = tuples[s:s+BATCH]
#             ids = [t[0] for t in batch]
#             texts = [t[1] for t in batch]
#             metas = [t[2] for t in batch]
#             logger.debug("Embedding batch size: %d", len(texts))

#             vecs = embeddings.embed_documents(texts)  # ì„ë² ë”© ì§ì ‘ ê³„ì‚°
#             if not vecs:
#                 logger.error("Embedding funtion returned an empty list for texts batch size:%d",len(texts))
#             # ğŸ” í•µì‹¬: add() ì•„ë‹˜. upsert() ì‚¬ìš©
#             coll.upsert(ids=ids, documents=texts, metadatas=metas, embeddings=vecs)

#             done += len(batch)
#             # ê°€ë²¼ìš´ ì§„í–‰ ë¡œê·¸
#             if (s // BATCH) % 10 == 0:
#                 elapsed = time.perf_counter() - t0
#                 rps = done / elapsed if elapsed > 0 else 0.0
#                 logger.info("Upsert progress: %d/%d (%.1f%%) | %.1f docs/s | %.1fs elapsed",
#                             done, total, done/total*100.0, rps, elapsed)

#         elapsed = time.perf_counter() - t0
#         logger.info("Vectorstore upsert done: %d docs in %.2fs (%.1f docs/s)",
#                     done, elapsed, done/elapsed if elapsed > 0 else 0.0)

#     # 3) ê²€ìƒ‰ìš© ë˜í¼ ë°˜í™˜ (ì´í›„ similarity_search ë“± ê·¸ëŒ€ë¡œ ì‚¬ìš©)
#     vs = Chroma(
#         client=client,
#         collection_name=idx_cfg.collection,
#         embedding_function=embeddings
#     )
#     return vs



from __future__ import annotations

import time
import logging
from typing import List

import chromadb
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document

from ..config import ModelCfg, IndexCfg
from ..utils.ids import make_chunk_id

logger = logging.getLogger(__name__)


def build_vectorstore(docs: List[Document], model_cfg: ModelCfg, idx_cfg: IndexCfg) -> Chroma:
    """
    - LangChain Chroma ë˜í¼ + Chroma Native upsert ì‚¬ìš©
    - chunk_id ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
    - ë¹ˆ í…ìŠ¤íŠ¸/ë¹ˆ ë°°ì¹˜ ë°©ì–´
    """
    # 1) Chroma client & collection
    client = chromadb.PersistentClient(path=str(idx_cfg.chroma_dir))
    coll = client.get_or_create_collection(name=idx_cfg.collection)

    # 2) Embedding ì¤€ë¹„
    embeddings = HuggingFaceEmbeddings(
        model_name=model_cfg.embed_model,
        encode_kwargs={"normalize_embeddings": True},
    )

    if docs:
        BATCH = 128
        t0 = time.perf_counter()

        logger.info(
            "Vectorstore upsert start: docs=%d batch=%d collection=%s dir=%s",
            len(docs),
            BATCH,
            idx_cfg.collection,
            idx_cfg.chroma_dir,
        )

        # --- Document -> (id, text, metadata) ë³€í™˜ + ì¤‘ë³µ/ë¹ˆí…ìŠ¤íŠ¸ ì œê±° ---
        def _to_tuple(d: Document):
            meta = dict(getattr(d, "metadata", {}) or {})
            source = meta.get("source") or meta.get("doc_id") or "unknown"
            page = meta.get("page")
            text = (d.page_content or "").strip()

            if not text:
                # ë‚´ìš©ì´ ì™„ì „íˆ ë¹„ì–´ìˆìœ¼ë©´ None ë°˜í™˜í•´ì„œ ìŠ¤í‚µ
                return None

            _id = make_chunk_id(str(source), page, text)
            meta.setdefault("chunk_id", _id)
            return _id, text, meta

        seen_ids = set()
        tuples: List[tuple[str, str, dict]] = []

        for d in docs:
            t = _to_tuple(d)
            if t is None:
                continue
            _id, text, meta = t
            if _id in seen_ids:
                # logger.debug("Duplicate chunk id skip: %s", _id)
                continue
            seen_ids.add(_id)
            tuples.append((_id, text, meta))

        total = len(tuples)
        print(f"[VS] unique chunks={total} (from docs={len(docs)})")

        done = 0
        if total == 0:
            logger.warning("No unique non-empty chunks to upsert; skipping Chroma upsert.")
        else:
            for s in range(0, total, BATCH):
                batch = tuples[s:s + BATCH]
                if not batch:
                    # ë¹ˆ ë°°ì¹˜ëŠ” ë°”ë¡œ ìŠ¤í‚µ (ì¤‘ë³µ ì œê±° ì´í›„ tailì—ì„œ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ)
                    continue

                ids = [t[0] for t in batch]
                texts = [t[1] for t in batch]
                metas = [t[2] for t in batch]

                # í˜¹ì‹œ ëª¨ë¥¼ ë¹ˆ í…ìŠ¤íŠ¸ ë°°ì¹˜ ë°©ì–´
                if not texts:
                    print(f"[VS] skip batch at offset {s} (empty texts)")
                    continue

                vecs = embeddings.embed_documents(texts)
                if not vecs:
                    print(
                        f"[VS] Embedding function returned empty list at offset {s}, "
                        f"texts_len={len(texts)}; skipping this batch."
                    )
                    continue

                coll.upsert(ids=ids, documents=texts, metadatas=metas, embeddings=vecs)
                done += len(batch)

                if (s // BATCH) % 10 == 0:
                    elapsed = time.perf_counter() - t0
                    rps = done / elapsed if elapsed > 0 else 0.0
                    logger.info(
                        "Upsert progress: %d/%d (%.1f%%) | %.1f docs/s | %.1fs elapsed",
                        done,
                        total,
                        done / total * 100.0,
                        rps,
                        elapsed,
                    )

        elapsed = time.perf_counter() - t0
        logger.info(
            "Vectorstore upsert done: %d docs in %.2fs (%.1f docs/s)",
            done,
            elapsed,
            done / elapsed if elapsed > 0 else 0.0,
        )

    # 3) ê²€ìƒ‰ìš© ë˜í¼ ë°˜í™˜
    vs = Chroma(
        client=client,
        collection_name=idx_cfg.collection,
        embedding_function=embeddings,
    )
    return vs
