from __future__ import annotations

import csv
import logging
from pathlib import Path
from typing import List

import pandas as pd
from langchain_core.documents import Document
# from langchain_text_splitters import RecursiveCharacterTextSplitter í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„°
from langchain_experimental.text_splitter import SemanticChunker
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.language_models import BaseLanguageModel
from langchain_community.document_loaders import PyMuPDFLoader

from rag_pipeline.config import ChunkCfg, PipelineCfg, ModelCfg
from rag_pipeline.data_io.csv_schema import (
    FIELD_ALIASES,
    meta_get,
    make_csv_schema_report,
    print_csv_schema_report,
    save_csv_schema_report,
)
from rag_pipeline.data_io.readers import read_txt
from rag_pipeline.utils.text import extract_kv_metadata
from rag_pipeline.metadata.tag_extractor import extract_metadata_from_text

logger = logging.getLogger(__name__)

def _sanitize_metadata(meta: dict) -> dict:
    """
    Chromaì— ë„£ê¸° ì „ì— metadata ê°’ì„ primitive íƒ€ì…ìœ¼ë¡œ ì •ë¦¬.
    - str, int, float, bool, None: ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - list, tuple: ", "ë¡œ joiní•´ì„œ ë¬¸ìì—´ë¡œ ë³€í™˜
    - ê·¸ ì™¸ íƒ€ì…: str()ìœ¼ë¡œ ë³€í™˜
    """
    clean: dict = {}
    for k, v in meta.items():
        if isinstance(v, (str, int, float, bool)) or v is None:
            clean[k] = v
        elif isinstance(v, (list, tuple)):
            # tags ê°™ì€ ê²½ìš°: ['ê°€ì„¤ê³µì‚¬', 'ë™ë°”ë¦¬', ...] â†’ "ê°€ì„¤ê³µì‚¬, ë™ë°”ë¦¬, ..."
            clean[k] = ", ".join(str(x) for x in v)
        else:
            clean[k] = str(v)
    return clean

def _try_read_csv(path):
    # 1) íŒŒì¼ ì•ë¶€ë¶„ ìƒ˜í”Œ
    raw = Path(path).read_bytes()
    head = raw[:4096]

    # 2) í›„ë³´ ì¸ì½”ë”©/êµ¬ë¶„ì
    encodings = ["cp949", "utf-8", "utf-8-sig", "euc-kr", "ISO-8859-1"]
    seps = [",", ";", "\t"]     # ì½¤ë§ˆ/ì„¸ë¯¸ì½œë¡ /íƒ­

    # 3) pandas ìœ ì—° ì˜µì…˜
    common_kwargs = dict(
        engine="python",           # ë” ê´€ëŒ€í•¨
        on_bad_lines="skip",       # ê¹¨ì§„ ë¼ì¸ ê±´ë„ˆëœ€ (pandas>=1.5)
        dtype=str,                 # ëª¨ë“  ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ
        quoting=csv.QUOTE_MINIMAL, # ë”°ì˜´í‘œ ì²˜ë¦¬
    )

    # 4) sep ìë™ ì¶”ì • 1ì°¨ (csv.Sniffer)
    sniff_sep = None
    try:
        sample = head.decode("utf-8", errors="ignore")
        sniff = csv.Sniffer().sniff(sample, delimiters=";,\t")
        sniff_sep = sniff.delimiter
    except Exception:
        pass

    # 5) ì‹œë„ ìˆœì„œ: (ì¶”ì • sep + ê° ì¸ì½”ë”©) â†’ (seps ì „ìˆ˜ + ê° ì¸ì½”ë”©) â†’ (sep=None)
    # 5-1) ì¶”ì • sepê°€ ìˆìœ¼ë©´ ë¨¼ì € ì‹œë„
    if sniff_sep:
        for enc in encodings:
            try:
                logger.debug("CSV sniff attempt: path=%s sep=%r enc=%s", path, sniff_sep, enc)
                return pd.read_csv(path, encoding=enc, sep=sniff_sep, **common_kwargs)
            except Exception as e:
                logger.debug(
                    "CSV sniff failed: path=%s sep=%r enc=%s error=%s",
                    path,
                    sniff_sep,
                    enc,
                    e,
                )

    # 5-2) ëŒ€í‘œ êµ¬ë¶„ìë“¤ ì „ìˆ˜ ì‹œë„
    for sep in seps:
        for enc in encodings:
            try:
                logger.debug("CSV attempt: path=%s sep=%r enc=%s", path, sep, enc)
                return pd.read_csv(path, encoding=enc, sep=sep, **common_kwargs)
            except Exception as e:
                logger.debug(
                    "CSV failed: path=%s sep=%r enc=%s error=%s",
                    path,
                    sep,
                    enc,
                    e,
                )
                # pass

    # 5-3) ë§ˆì§€ë§‰ ì‹œë„: sep ìë™(None) + ì¸ì½”ë”© ì „ìˆ˜
    for enc in encodings:
        try:
            logger.debug("CSV fallback attempt: path=%s sep=auto enc=%s", path, enc)
            return pd.read_csv(path, encoding=enc, sep=None, **common_kwargs)
        except Exception as e:
            logger.debug(
                "CSV fallback failed: path=%s sep=auto enc=%s error=%s",
                path,
                enc,
                e,
            )
            pass

    logger.error("CSV parsing failed for %s", path)
    return None

# ë…¸ì´ì¦ˆ ì œê±°
def csv_rows_to_documents(path: Path, pipeline_cfg: PipelineCfg) -> List[Document]:
    df = _try_read_csv(path)
    if df is None or df.empty:
        raise RuntimeError(f"Failed to read CSV (encoding/sep/lines): {path}")

    df.columns = [str(c).strip() for c in df.columns]

    report = make_csv_schema_report(df, path)
    print_csv_schema_report(report)
    save_csv_schema_report(
        report,
        str(pipeline_cfg.schema_report_dir) if pipeline_cfg.schema_report_dir else None,
    )

    # FIELD_ALIASESì˜ logical key ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ, ì‹¤ì œ ì»¬ëŸ¼ëª…ì€ meta_getì´ ì•Œì•„ì„œ ë§¤í•‘í•œë‹¤.
    logical_keys = list(FIELD_ALIASES.keys())

    docs: List[Document] = []
    for i, (_, row) in enumerate(df.iterrows()):
        # 1) ì›ë³¸ rowë¥¼ ê·¸ëŒ€ë¡œ ë©”íƒ€ë°ì´í„°ì— ë„£ëŠ”ë‹¤ (íŒŒì¼ì— ìˆëŠ” ì»¬ëŸ¼ëª… ê·¸ëŒ€ë¡œ).
        md = {str(k).strip(): ("" if pd.isna(v) else str(v)) for k, v in row.items()}

        # 2) ë…¼ë¦¬ í‚¤ ê¸°ì¤€ìœ¼ë¡œ ê°’ ì¶”ì¶œ (alias í¬í•¨).
        body_lines: list[str] = []
        normalized_meta: dict = {}
        for key in logical_keys:
            val = meta_get(md, key)
            if val and val != "ì •ë³´ ì—†ìŒ":
                body_lines.append(f"{key}: {val}")
                # ë©”íƒ€ë°ì´í„°ì—ë„ í‘œì¤€í™”ëœ í‚¤ë¡œ í•œ ë²ˆ ë” ì €ì¥í•´ ë‘”ë‹¤.
                normalized_meta[key] = val

        body = "\n".join(body_lines).strip()

        # 3) ìµœì¢… ë©”íƒ€ë°ì´í„°: source + ì›ë³¸ ì»¬ëŸ¼ + í‘œì¤€í™” í‚¤ + ê³ ì • id
        meta = {
            "source": str(path),
            **md,
            **normalized_meta,
        }
        meta["id"] = f"{meta['source']}#p0#c{i}"

        docs.append(
            Document(
                page_content=body or str(md),
                metadata=meta,
            )
        )

    return docs

def load_documents_from_path(path: Path, chunk_cfg: ChunkCfg, pipeline_cfg: PipelineCfg,
                             llm_for_tags: BaseLanguageModel | None = None) -> List[Document]:
    if path.is_dir():
        docs: List[Document] = []
        for p in path.rglob("*"):
            if p.is_file() and p.suffix.lower() in {".csv", ".txt", ".pdf"}:
                logger.info("Loading file for chunking: %s", p)
                docs.extend(load_documents_from_path(p, chunk_cfg, pipeline_cfg, llm_for_tags=llm_for_tags))
        return docs
    if path.suffix.lower() == ".csv":
        return csv_rows_to_documents(path, pipeline_cfg)
    elif path.suffix.lower() in {".txt", ".pdf"}:
        return generic_file_to_documents(path, chunk_cfg,llm_for_tags=llm_for_tags)
    return []

# ì‹œë©˜í‹± ì²­í‚¹ ì ìš© ë²„ì „
def generic_file_to_documents(
    path: Path,
    chunk_cfg: ChunkCfg,
    llm_for_tags: BaseLanguageModel | None = None,
) -> List[Document]:
    print(f"[CHUNK] generic_file_to_documents start: {path}")
    """
    TXT/PDF ë“± ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì¼ì„ 'ì‹œë©˜í‹± ì²­í‚¹(ì˜ë¯¸ ê¸°ë°˜)'ìœ¼ë¡œ ë³€í™˜.
    """

    # 1) ì›ë³¸ í…ìŠ¤íŠ¸ ì½ê¸°
    ext = path.suffix.lower()
    if ext == ".txt":
        raw = read_txt(path)
    elif ext == ".pdf":
        loader = PyMuPDFLoader(str(path))
        pages = loader.load()
        raw = "\n".join(p.page_content for p in pages)
        print(f"[PDF] PyMuPDFLoader loaded {len(pages)} pages from {path.name}")
    else:
        return []

    if not raw or not raw.strip():
        return []

    # 2) ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
    base_meta: dict = {
        "source": str(path),
    }

    # 3) LLM íƒœê¹… (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    if llm_for_tags is not None:
        try:
            tag_meta = extract_metadata_from_text(
                text=raw[:4000],
                filename=path.name,
                llm=llm_for_tags,
            )
        except Exception as e:
            print(f"[TAG] ë©”íƒ€ë°ì´í„°/íƒœê·¸ ì¶”ì¶œ ì‹¤íŒ¨: file={path} | err={e}")
            tag_meta = {}
        else:
            base_meta.update({
                "document_type": tag_meta.get("document_type"),
                "project_name": tag_meta.get("project_name"),
                "location": tag_meta.get("location"),
                "company": tag_meta.get("company"),
                "facility_type": tag_meta.get("facility_type"),
                "tags": tag_meta.get("tags", []),
                "source_filename": tag_meta.get("source_filename", path.name),
            })

    # 4) í‚¤:ê°’ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    kv_meta = extract_kv_metadata(raw)
    base_meta.update({f"meta:{k}": v for k, v in kv_meta.items()})

    # -------------------------------------------------------------------------
    # ğŸ”¥ [í•µì‹¬ ìˆ˜ì •] ì‹œë©˜í‹± ì²­ì»¤ ì ìš©
    # -------------------------------------------------------------------------
    print(f"[CHUNK] ì‹œë©˜í‹± ì²­í‚¹ ì‹œì‘... (Embedding ì—°ì‚°ìœ¼ë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    
    # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (config.pyì˜ ModelCfg ì‚¬ìš©)
    model_cfg = ModelCfg()
    embeddings = HuggingFaceEmbeddings(
        model_name=model_cfg.embed_model,
        encode_kwargs={"normalize_embeddings": True}
    )

    # ì‹œë©˜í‹± ì²­ì»¤ ì´ˆê¸°í™”
    # breakpoint_threshold_type: "percentile"(ê¸°ë³¸ê°’), "standard_deviation", "interquartile" ë“± ì„ íƒ ê°€ëŠ¥
    splitter = SemanticChunker(
        embeddings=embeddings,
        breakpoint_threshold_type="percentile", # ì˜ë¯¸ ë³€í™”ê°€ í° ìƒìœ„ ì§€ì ì„ ìë¦„
        breakpoint_threshold_amount=90,         # ë¯¼ê°ë„ ì¡°ì ˆ (ë†’ì„ìˆ˜ë¡ ëœ ìë¦„)
    )
    
    # í…ìŠ¤íŠ¸ ë¶„í•  ì‹¤í–‰
    try:
        chunks = splitter.split_text(raw)
    except Exception as e:
        print(f"[CHUNK] ì‹œë©˜í‹± ì²­í‚¹ ì‹¤íŒ¨, ê¸°ë³¸ ìŠ¤í”Œë¦¬í„°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤: {e}")
        # ì‹¤íŒ¨ ì‹œ fallback (ê¸°ì¡´ ë°©ì‹)
        from langchain_text_splitters import RecursiveCharacterTextSplitter
        fallback_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_cfg.size,
            chunk_overlap=chunk_cfg.overlap,
        )
        chunks = fallback_splitter.split_text(raw)

    print(
        f"[CHUNK] file={path.name} ext={path.suffix.lower()} "
        f"semantic_chunks={len(chunks)}"
    )

    # 6) Document ë¦¬ìŠ¤íŠ¸ ìƒì„±
    docs: List[Document] = []
    for i, ch in enumerate(chunks):
        # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ì€ ì²­í¬(ë…¸ì´ì¦ˆ)ëŠ” ìŠ¤í‚µ
        if len(ch.strip()) < 10:
            continue

        raw_meta = {
            **base_meta,
            "chunk_id": i,
            "id": f"{base_meta['source']}#p0#c{i}",
        }
        safe_meta = _sanitize_metadata(raw_meta)

        docs.append(
            Document(
                page_content=ch,
                metadata=safe_meta,
            )
        )

    return docs

