from rag_pipeline.config import ModelCfg
from rag_pipeline.retrieval.hybrid import hybrid_retrieve
from rag_pipeline.retrieval.rerankers import CrossEncoderReranker
from rag_pipeline.indexing.sparse import build_sparse_retriever
from rag_pipeline.llm.summarize import summarize_with_llm
from rag_pipeline.llm.json_answer import answer_with_json_autoschema
from rag_pipeline.core.global_index import get_global_index
from rag_pipeline.core.workspace import WORKSPACES
from rag_pipeline.core.processing import VS_CACHE, BM25_CACHE, DOCS_CACHE
from fastapi import HTTPException
from langchain_core.documents import Document
import logging
from typing import Any
logger = logging.getLogger(__name__)


# ---------------------------
#  Helper: tags parser
# ---------------------------
def _ensure_tags_list(raw):
    """
    metadata['tags'] ê°’ì´
    - string: "ê°€ì„¤ê³µì‚¬, ë¹„ê³„, ì¶”ë½ ìœ„í—˜"
    - list: ["ê°€ì„¤ê³µì‚¬","ë¹„ê³„"]
    ì–´ëŠ í˜•íƒœë“  list[str] ë¡œ ë³€í™˜.
    """
    if isinstance(raw, str):
        return [t.strip() for t in raw.split(",") if t.strip()]
    if isinstance(raw, list):
        return raw
    return []

CASE_QUERY_STOPWORDS = [
    "ì‚¬ê³ ", "ì‚¬ë¡€", "ì‚¬ê³ ì‚¬ë¡€", "ì‚¬ê³  ì‚¬ë¡€",
    "ì•Œë ¤ì¤˜", "ì•Œë ¤ ì¤˜", "ì•Œë ¤ì¤˜ìš”",
    "ì— ëŒ€í•œ", "ì—ëŒ€í•œ", "ì— ê´€í•´", "ì—ê´€í•´",
    "ì„", "ë¥¼", "ì´", "ê°€", "ì€", "ëŠ”"
]

def _answer_to_text(answer: Any) -> str:
    """
    JSON Auto-Schemaì˜ answer ê°ì²´ë¥¼ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ í’€ì–´ì£¼ëŠ” ìœ í‹¸.
    êµ¬ì¡°ê°€ ë§¤ë²ˆ ë‹¬ë¼ë„ ìµœëŒ€í•œ ì˜ˆì˜ê²Œ í¼ì³ì„œ í•œê¸€ ìš”ì•½ì²˜ëŸ¼ ë³´ì—¬ì£¼ê¸° ìœ„í•¨.
    """
    if answer is None:
        return ""

    # 1) ì‚¬ê³  ì˜ˆì‹œì²˜ëŸ¼ ë‹¨ìˆœ dict ì¸ ê²½ìš°
    if isinstance(answer, dict):
        lines = []
        for k, v in answer.items():
            # ê¸°ë³¸ íƒ€ì…ì€ "í‚¤: ê°’" í˜•íƒœë¡œ
            if isinstance(v, (str, int, float, bool)):
                lines.append(f"{k}: {v}")
            # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ì˜ˆ: cases ë¦¬ìŠ¤íŠ¸ ë“±)
            elif isinstance(v, list):
                # ë¦¬ìŠ¤íŠ¸ ì•ˆì— dictë“¤ì´ ë“¤ì–´ìˆëŠ” ê²½ìš° ì²« ë²ˆì§¸ë§Œ ê°„ë‹¨ ìš”ì•½
                if v and isinstance(v[0], dict):
                    lines.append(f"{k}:")
                    first = v[0]
                    for kk, vv in first.items():
                        if isinstance(vv, (str, int, float, bool)):
                            lines.append(f"  - {kk}: {vv}")
                else:
                    # ë‹¨ìˆœ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ ë“±
                    joined = ", ".join(map(str, v))
                    lines.append(f"{k}: {joined}")
            else:
                # ê·¸ ë°–ì˜ íƒ€ì…ë“¤ì€ ë¬¸ìì—´ë¡œ ê·¸ëƒ¥ ë˜ì§
                lines.append(f"{k}: {str(v)}")
        return "\n".join(lines)

    # 2) ë¦¬ìŠ¤íŠ¸ ì „ì²´ê°€ answerì¸ ê²½ìš°
    if isinstance(answer, list):
        parts = []
        for idx, item in enumerate(answer, start=1):
            if isinstance(item, dict):
                parts.append(f"[{idx}ë²ˆ í•­ëª©]")
                for k, v in item.items():
                    parts.append(f"- {k}: {v}")
            else:
                parts.append(f"- {item}")
        return "\n".join(parts)

    # 3) ê·¸ ì™¸ëŠ” ê·¸ëƒ¥ ë¬¸ìì—´ë¡œ ìºìŠ¤íŒ…
    return str(answer)


def extract_query_keywords(query: str | None) -> list[str]:
    """
    ì§ˆë¬¸ì—ì„œ 'ìˆœì˜ì¢…í•©ê±´ì„¤', 'ìŠ¤íƒ€í•„ë“œì•ˆì„±', 'ì§€ì‹ì‚°ì—…ì„¼íƒ€' ê°™ì€
    ê³ ìœ ëª…ì‚¬/í•µì‹¬ í‚¤ì›Œë“œë§Œ ëŒ€ëµ ë½‘ëŠ”ë‹¤.
    'ìš°ì„ ìˆœìœ„ íŒíŠ¸' ë¡œ ì‚¬ìš©
    """
    if not query:
        return []
    q = query.strip()
    if not q:
        return []
    
    parts = q.split()
    keywords: list[str] = []
    for part in parts:
        part = part.strip()
        if not part:
            continue
        
        if any(sw in part for sw in CASE_QUERY_STOPWORDS):
            continue
        if len(part) < 2:
            continue
        
        keywords.append(part)
        
    return keywords

def prioritize_docs_by_keywords(docs: list[Document], keywords: list[str]) -> list[Document]:
    """
    ë¬¸ì„œë¥¼ ë²„ë¦¬ì§€ ì•Šê³ , í‚¤ì›Œë“œê°€ ë“¤ì–´ìˆëŠ” ë¬¸ì„œë§Œ ì•ìœ¼ë¡œ ì •ë ¬
    - ë§¤ì¹­ëœ ë¬¸ì„œë“¤ ë¨¼ì €
    - ë‚˜ë¨¸ì§€ ë¬¸ì„œë“¤ ê·¸ ë’¤ì— ê·¸ëŒ€ë¡œ
    """
    if not keywords or not docs:
        return docs
    
    hits: list[Document] = []
    others: list[Document] = []
    
    for d in docs:
        md = d.metadata or {}
        meta_values = [str(v) for v in md.values() if v is not None]
        text_pieces = meta_values + [
            str(md.get("source", "") or ""),
            str(d.page_content or ""),
        ]
        big_text = " ".join(text_pieces)
        
        if any(kw in big_text for kw in keywords):
            hits.append(d)
        else:
            others.append(d)
            
    # ë¬¸ì„œ í•˜ë‚˜ë„ ë§¤ì¹­ ì•ˆ ë˜ë©´, ìˆœì„œ ì•ˆ ê±´ë“œë¦¼ 
    if not hits:
        return docs
    
    # í‚¤ì›Œë“œê°€ ë“¤ì–´ìˆëŠ” ì• ë“¤ì„ ì•ìœ¼ë¡œ ë‚˜ë¨¸ì§€ëŠ” ê·¸ëŒ€ë¡œ ë’¤ì—
    return hits + others

def run_pipeline(req):

    model_cfg = ModelCfg()

    # ğŸ”¹ LLM ëª¨ë¸ ì„ íƒ
    llm_choise = getattr(req, "llm_model", None) or "gemma3:27b"
    print(f"[LLM MODEL] : {llm_choise}")

    # ì§ˆì˜ ì •ë³´ ë¡œê·¸
    print(f"[QUERY] query={req.query} | workspace={req.workspace_id} | tags={req.tags}")

    reranker = CrossEncoderReranker(model_cfg.rerank_model) if req.use_rerank else None

    vs = bm25 = docs = None
    ws_id = getattr(req, "workspace_id", None)

    # -----------------------------------------------------
    # 1) workspace ìºì‹œ ê¸°ë°˜ ë¡œë“œ
    # -----------------------------------------------------
    if ws_id:
        ws = WORKSPACES.get(ws_id)
        if not ws:
            raise HTTPException(400, "ìœ íš¨í•˜ì§€ ì•Šì€ workspace_id ì…ë‹ˆë‹¤.")

        vs = VS_CACHE.get(ws_id)
        bm25 = BM25_CACHE.get(ws_id)
        docs = DOCS_CACHE.get(ws_id)

        print(
            f"[DEBUG] ws_id={ws_id}, "
            f"vs_in_cache={ws_id in VS_CACHE}, "
            f"bm25_in_cache={ws_id in BM25_CACHE}, "
            f"docs_in_cache={ws_id in DOCS_CACHE}, "
            f"docs_len={len(docs) if docs else 0}"
        )

    # -----------------------------------------------------
    # 2) ì „ì—­ ì¸ë±ìŠ¤ fallback
    # -----------------------------------------------------
    if vs is None and bm25 is None and docs is None:
        try:
            g_vs, g_bm25, g_docs = get_global_index()
            vs = g_vs or vs
            bm25 = g_bm25 or bm25
            docs = g_docs or docs
            print(
                f"[DEBUG] global_index: vs={bool(vs)}, "
                f"bm25={bool(bm25)}, docs_len={len(docs) if docs else 0}"
            )
        except Exception:
            print("[DEBUG] get_global_index() ì‹¤íŒ¨, ì „ì—­ ì¸ë±ìŠ¤ ì‚¬ìš© ì•ˆ í•¨")

    # -----------------------------------------------------
    # 3) BM25 lazy build
    # -----------------------------------------------------
    if bm25 is None and docs:
        bm25 = build_sparse_retriever(docs)
        if ws_id:
            BM25_CACHE[ws_id] = bm25
        print(f"[DEBUG] lazy build bm25, docs={len(docs)}")

    # -----------------------------------------------------
    # 4) retrieval (dense / sparse / hybrid)
    # -----------------------------------------------------
    used_vs = False
    used_bm25 = False

    mode = (req.retrieval or "hybrid").lower()

    if mode == "dense":
        matched = vs.as_retriever(search_kwargs={"k": req.k}).invoke(req.query) if vs else []
        used_vs = True

    elif mode == "sparse":
        if bm25:
            bm25.k = req.k
            matched = bm25.invoke(req.query)
            used_bm25 = True
        else:
            matched = []

    else:  # hybrid
        if vs and bm25:
            matched = hybrid_retrieve(
                req.query,
                vs,
                bm25,
                k_dense=req.k,
                k_sparse=req.k,
                k_final=req.final_k,
                reranker=reranker,
            )
            used_vs = True
            used_bm25 = True
        elif vs:
            matched = vs.as_retriever(search_kwargs={"k": req.k}).invoke(req.query)
            used_vs = True
        elif bm25:
            bm25.k = req.k
            matched = bm25.invoke(req.query)
            used_bm25 = True
        else:
            matched = []

    print(f"[DEBUG] mode={mode}, vs={used_vs}, bm25={used_bm25}, matched_len={len(matched)}")
    print(
        f"[DEBUG] query={req.query!r}, mode={mode}, "
        f"k={req.k}, final_k={req.final_k}, use_rerank={req.use_rerank}, "
    )

    # -----------------------------------------------------
    # 5) ë§¤ì¹­ëœ ë¬¸ì„œë“¤ì˜ íƒœê·¸ ë¶„í¬ ì¶œë ¥
    # -----------------------------------------------------
    all_tags = []
    for d in matched:
        all_tags.extend(_ensure_tags_list(d.metadata.get("tags")))

    unique_tags = sorted(set(all_tags))
    print(f"[MATCH] matched_docs={len(matched)} | unique_tags={unique_tags}")

    if matched:
        print("[DEBUG] first matched metadata:", matched[0].metadata)

    # -----------------------------------------------------
    # 6) ìš”ì²­ì—ì„œ tags í•„í„°ë§
    # -----------------------------------------------------
    req_tags = getattr(req, "tags", None)
    if req_tags:
        required_tags = set(req_tags)
        before = len(matched)

        matched = [
            d for d in matched
            if required_tags.intersection(_ensure_tags_list(d.metadata.get("tags")))
        ]

        print(f"[TAG-FILTER] required={list(required_tags)} | before={before} -> after={len(matched)}")

    # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)
    if not matched:
        return {
            "summary": "ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            "sources": [],
        }

    # ----------------------------------------------------
    # 6.5) í‚¤ì›Œë“œ ê¸°ë°˜ 'ìˆœì„œë§Œ' ì¡°ì • (ë¬¸ì„œ ë²„ë¦¬ì§€ ì•ŠìŒ)
    # ----------------------------------------------------
    keywords = extract_query_keywords(getattr(req, "query", None))
    ordered = prioritize_docs_by_keywords(matched, keywords)

    print(
        f"[LLM MODEL] : {llm_choise}"
        f"[ORDER] keywords={keywords} | before={len(matched)} | "
        f"first_changed={matched[0] is not ordered[0] if matched and ordered else False}"
    )

    # ğŸ”¹ LLMì— ë„˜ê¸¸ ìµœì¢… ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ì •ë ¬ ì ìš©, ì‹¤íŒ¨ ì‹œ fallback)
    final_docs = ordered or matched

    # -----------------------------------------------------
    # 7) LLM JSON Auto-Schema ì‘ë‹µ ìƒì„±
    # -----------------------------------------------------

    # âœ… JSON Mode + Auto-Schema í˜¸ì¶œ
    json_answer = answer_with_json_autoschema(
        query=req.query,
        docs=final_docs,
        model_cfg=model_cfg,
        llm_model=llm_choise,   # ìš”ì²­ì—ì„œ ë°›ì€ LLM ì„ íƒ ë°˜ì˜
    )

    # -----------------------------------------------------
    # 8) ë°˜í™˜ (ê¸°ì¡´ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ìœ ì§€ + JSON Auto-Schema ì¶”ê°€)
    # -----------------------------------------------------

    # summary í•„ë“œëŠ” response ëª¨ë¸ì´ ìš”êµ¬í•˜ë‹ˆê¹Œ, ê°„ë‹¨íˆ schema.descriptionì„ ì‚¬ìš©
    summary_text = json_answer.get("schema", {}).get("description", "")

    return {
        "summary": summary_text,          # ğŸ”¹ FastAPI ì‘ë‹µ ìŠ¤í‚¤ë§ˆìš© (string)
        "mode": mode,
        "llm_model": llm_choise,
        "json": json_answer,              # ğŸ”¹ ìƒˆ JSON Auto-Schema ì „ì²´
        "sources": [
            {
                "source": d.metadata.get("source", ""),
                "doc_id": d.metadata.get("doc_id") or d.metadata.get("source") or "",
                "chunk_id": d.metadata.get("chunk_id") or d.metadata.get("id") or "",
            }
            for d in final_docs
        ],
    }

